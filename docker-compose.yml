services:
  trtllm:
    image: ${TRTLLM_IMAGE}
    container_name: trtllm_llm_server
    gpus: all
    ipc: host
    network_mode: host
    environment:
    - HF_TOKEN=${HF_TOKEN}
    # Work around occasional XetHub transfer 403s by forcing standard Hugging Face downloads.
    - HF_HUB_DISABLE_XET=1
    - HF_HUB_ENABLE_HF_TRANSFER=0
    - MODEL_HANDLE=${MODEL_HANDLE}
    - HOST=${HOST}
    - PORT=${PORT}
    # Optional proxies if set in .env
    - HTTPS_PROXY=${HTTPS_PROXY}
    - HTTP_PROXY=${HTTP_PROXY}
    - NO_PROXY=${NO_PROXY}
    volumes:
    # Persist HF downloads on the host for faster restarts
    - ${HF_CACHE_DIR}:/root/.cache/huggingface
    # Extra config mounted read-only
    - ./config/extra-llm-api-config.yml:/etc/trtllm/extra-llm-api-config.yml:ro
    command: >
      bash -lc '
        set -euo pipefail

        echo "[trtllm] MODEL_HANDLE=$$MODEL_HANDLE"
        echo "[trtllm] Binding on $${HOST}:$${PORT}"
        echo "[trtllm] HF cache: /root/.cache/huggingface"

        # Tokenizer encoding files used by OpenAI tokenizer families (kept local in container)
        export TIKTOKEN_ENCODINGS_BASE="/tmp/harmony-reqs"
        mkdir -p "$$TIKTOKEN_ENCODINGS_BASE"
        wget -q -P "$$TIKTOKEN_ENCODINGS_BASE" https://openaipublic.blob.core.windows.net/encodings/o200k_base.tiktoken
        wget -q -P "$$TIKTOKEN_ENCODINGS_BASE" https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken

        # Download only what trtllm-serve needs into HF cache (no-op if already cached)
        # This avoids pulling large optional artifacts like `original/*` and `metal/model.bin`.
        hf download "$$MODEL_HANDLE" \
          --include \
            "model-*.safetensors" \
            "model.safetensors.index.json" \
            "config.json" \
            "generation_config.json" \
            "tokenizer.json" \
            "tokenizer_config.json" \
            "special_tokens_map.json" \
            "chat_template.jinja" \
            "README.md" \
            "LICENSE" \
            "USAGE_POLICY" \
            ".gitattributes"

        # Force offline usage for serving so we do not stall on optional network fetches.
        export HF_HUB_OFFLINE=1
        export TRANSFORMERS_OFFLINE=1

        # Start server
        trtllm-serve serve "$$MODEL_HANDLE" \
          --trust_remote_code \
          --host "$$HOST" \
          --port "$$PORT" \
          --num_postprocess_workers 4 \
          --max_batch_size 64 \
          --extra_llm_api_options /etc/trtllm/extra-llm-api-config.yml
      '
    restart: unless-stopped

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open_webui
    restart: unless-stopped
    ports:
    - "3000:8080"
    extra_hosts:
    - "host.docker.internal:host-gateway"
    environment:
    # Pre-configure OpenAI-compatible endpoint (the TRT-LLM server on the host)
    - OPENAI_API_BASE_URL=http://host.docker.internal:${PORT}/v1
    # TRT-LLM server doesn't require auth by default; WebUI expects a non-empty key.
    - OPENAI_API_KEY=sk-local
    volumes:
    - open-webui:/app/backend/data

volumes:
  open-webui:
